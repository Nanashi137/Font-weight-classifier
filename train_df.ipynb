{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as L\n",
    "\n",
    "from pytorch_lightning.callbacks import (\n",
    "    EarlyStopping,\n",
    "    LearningRateMonitor,\n",
    "    ModelCheckpoint,\n",
    "    ModelSummary,\n",
    ")\n",
    "\n",
    "import torch\n",
    "\n",
    "from model.deepfont import DeepFont\n",
    "from model.lightning_generate_callback import GenerateCallback\n",
    "from model.lightning_wrappers import DeepFontAutoencoderWrapper, DeepFontWrapper\n",
    "\n",
    "from dataset.dataset import FontWeightDataset, FWDataset\n",
    "from dataset.transformations import *\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from model.configs import load_config\n",
    "import numpy as np\n",
    "import random \n",
    "import os \n",
    "import cv2 as cv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_config(\"F:/FWC216/configs/adobe-vfr.yaml\")\n",
    "#config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load raw data\n",
    "data_path = \"data/\"\n",
    "folders = os.listdir(data_path)\n",
    "\n",
    "img_paths = []\n",
    "\n",
    "for folder in folders: \n",
    "    cpath = data_path + folder \n",
    "\n",
    "    images = os.listdir(cpath)\n",
    "    for image in images: \n",
    "        a_path = \"F:/FWC216/\" + data_path + folder + \"/\" + image\n",
    "        img_paths.append(a_path)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle and split data\n",
    "\n",
    "random.shuffle(img_paths) \n",
    "\n",
    "total_size = len(img_paths)\n",
    "train_size = int(0.7*total_size)\n",
    "test_size  = int(0.1*total_size) \n",
    "valid_size =  total_size - train_size - test_size\n",
    "\n",
    "train_data = img_paths[:train_size]\n",
    "test_data  = img_paths[train_size:train_size + test_size]\n",
    "valid_data = img_paths[train_size + test_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t = T1()\n",
    "# for path in train_data: \n",
    "#     img = cv.imread(path, cv.IMREAD_GRAYSCALE)\n",
    "#     img1 = Ptrans(img)\n",
    "#     img2 = t(image=img)['image']\n",
    "#     print(img1.shape, img2.shape)\n",
    "#     break\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t_img.shape\n",
    "# cv.imshow(\"n\", t_img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Torch dataset\n",
    "\n",
    "\n",
    "train_dataset =  FWDataset(img_folder = train_data)\n",
    "test_dataset  =  FWDataset(img_folder = test_data )\n",
    "valid_dataset =  FWDataset(img_folder = valid_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loader \n",
    "batch_size = 64\n",
    "train_loader =  DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=  4, persistent_workers= True)\n",
    "\n",
    "valid_loader =  DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, num_workers= 4, persistent_workers= True)\n",
    "\n",
    "test_loader  =  DataLoader(test_dataset,  batch_size=batch_size, shuffle=False, num_workers= 4, persistent_workers= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "torch.set_float32_matmul_precision(\"high\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "\n",
    "#Auto encoder \n",
    "ae = DeepFontAutoencoderWrapper()\n",
    "checkpoint = torch.load(\"output_ae/last.ckpt\")\n",
    "ae.load_state_dict(checkpoint['state_dict'])\n",
    "#DeepFont \n",
    "\n",
    "train_config = config[\"training\"]\n",
    "\n",
    "model = DeepFontWrapper(\n",
    "            model=DeepFont(\n",
    "                autoencoder= ae.autoencoder,\n",
    "                num_classes= 4,\n",
    "            ),\n",
    "            num_classes= 4,\n",
    "            learning_rate=train_config[\"learning_rate\"],\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "f:\\FWC\\lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:652: Checkpoint directory F:\\FWC216\\output_df exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name               | Type                | Params | Mode  | In sizes         | Out sizes       \n",
      "----------------------------------------------------------------------------------------------------------\n",
      "0  | model              | DeepFont            | 169 M  | train | [2, 1, 105, 105] | [2, 4]          \n",
      "1  | model.ae_encoder   | Sequential          | 83.5 K | train | [2, 1, 105, 105] | [2, 128, 12, 12]\n",
      "2  | model.ae_encoder.0 | Conv2d              | 9.3 K  | train | [2, 1, 105, 105] | [2, 64, 48, 48] \n",
      "3  | model.ae_encoder.1 | ReLU                | 0      | train | [2, 64, 48, 48]  | [2, 64, 48, 48] \n",
      "4  | model.ae_encoder.2 | BatchNorm2d         | 128    | train | [2, 64, 48, 48]  | [2, 64, 48, 48] \n",
      "5  | model.ae_encoder.3 | MaxPool2d           | 0      | train | [2, 64, 48, 48]  | [2, 64, 24, 24] \n",
      "6  | model.ae_encoder.4 | Conv2d              | 73.9 K | train | [2, 64, 24, 24]  | [2, 128, 24, 24]\n",
      "7  | model.ae_encoder.5 | ReLU                | 0      | train | [2, 128, 24, 24] | [2, 128, 24, 24]\n",
      "8  | model.ae_encoder.6 | BatchNorm2d         | 256    | train | [2, 128, 24, 24] | [2, 128, 24, 24]\n",
      "9  | model.ae_encoder.7 | MaxPool2d           | 0      | train | [2, 128, 24, 24] | [2, 128, 12, 12]\n",
      "10 | model.conv5        | Conv2d              | 295 K  | train | [2, 128, 12, 12] | [2, 256, 12, 12]\n",
      "11 | model.conv6        | Conv2d              | 590 K  | train | [2, 256, 12, 12] | [2, 256, 12, 12]\n",
      "12 | model.conv7        | Conv2d              | 590 K  | train | [2, 256, 12, 12] | [2, 256, 12, 12]\n",
      "13 | model.flatten      | Flatten             | 0      | train | [2, 256, 12, 12] | [2, 36864]      \n",
      "14 | model.fc1          | Linear              | 150 M  | train | [2, 36864]       | [2, 4096]       \n",
      "15 | model.drop1        | Dropout             | 0      | train | [2, 4096]        | [2, 4096]       \n",
      "16 | model.fc2          | Linear              | 16.8 M | train | [2, 4096]        | [2, 4096]       \n",
      "17 | model.drop2        | Dropout             | 0      | train | [2, 4096]        | [2, 4096]       \n",
      "18 | model.fc3          | Linear              | 16.4 K | train | [2, 4096]        | [2, 4]          \n",
      "19 | f1                 | MulticlassF1Score   | 0      | train | ?                | ?               \n",
      "20 | accuracy           | MulticlassAccuracy  | 0      | train | ?                | ?               \n",
      "21 | precision          | MulticlassPrecision | 0      | train | ?                | ?               \n",
      "22 | recall             | MulticlassRecall    | 0      | train | ?                | ?               \n",
      "----------------------------------------------------------------------------------------------------------\n",
      "169 M     Trainable params\n",
      "83.5 K    Non-trainable params\n",
      "169 M     Total params\n",
      "677.422   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|██████████| 438/438 [00:54<00:00,  8.06it/s, v_num=0, train_acc=1.000, train_loss=0.00319, train_f1=1.000, train_pre=1.000, train_rec=1.000, val_loss=0.0304, val_accuracy=0.989] \n"
     ]
    }
   ],
   "source": [
    "# Define trainer \n",
    "\n",
    "training_callbacks = [\n",
    "        EarlyStopping(monitor=\"val_loss\", mode=\"min\"),\n",
    "        LearningRateMonitor(logging_interval=\"step\"),\n",
    "        ModelCheckpoint(\n",
    "            dirpath=\"./output_df\",\n",
    "            save_top_k=2,\n",
    "            monitor=\"val_loss\",\n",
    "            filename=\"deepfont-{epoch:02d}-{val_loss:.4f}-{val_accuracy:.4f}\",\n",
    "            save_last=True,\n",
    "        ),\n",
    "        ModelSummary(-1),\n",
    "    ]\n",
    "\n",
    "\n",
    "\n",
    "trainer = L.Trainer(\n",
    "        # If ever doing gradient clipping here, remember that this\n",
    "        # has an effect on training: it will take likely twice as\n",
    "        # long to get to the same accuracy!\n",
    "        # gradient_clip_algorithm=\"norm\",\n",
    "        # gradient_clip_val=1.0,\n",
    "    max_epochs=40,\n",
    "    callbacks=training_callbacks,\n",
    ")\n",
    "\n",
    "\n",
    "trainer.fit(\n",
    "    model=model,\n",
    "    train_dataloaders=train_loader,\n",
    "    val_dataloaders=valid_loader,\n",
    "    ckpt_path= None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 63/63 [00:03<00:00, 19.51it/s]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test_acc            0.9902499914169312\n",
      "         test_f1            0.9902499914169312\n",
      "        test_loss          0.032494693994522095\n",
      "     test_precision         0.9902499914169312\n",
      "       test_recall          0.9902499914169312\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 0.032494693994522095,\n",
       "  'test_acc': 0.9902499914169312,\n",
       "  'test_f1': 0.9902499914169312,\n",
       "  'test_precision': 0.9902499914169312,\n",
       "  'test_recall': 0.9902499914169312}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filepath = \"fwc_model.onnx\"\n",
    "# model.to_onnx(file_path=filepath, export_params=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
